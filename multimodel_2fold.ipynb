{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Parameters\n",
    "fold = 0\n",
    "model_name = 'klue/roberta-base'\n",
    "BATCH_SIZE =64\n",
    "MAX_LEN =196\n",
    "MAX_WV_LEN = 4 * 16000\n",
    "EPOCHS = 30\n",
    "set_lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os \n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['neutral',\n",
    "    'angry',\n",
    "    'disgust',\n",
    "    'fear',\n",
    "    'happy',\n",
    "    'sad',\n",
    "    'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/workspace/ETRI/new_data.csv')\n",
    "df = df[['Numb','Segment ID','Total Evaluation','text','max_count']+ targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_info = {\"neutral\":0,\"angry\":1,\"disgust\":2,\"fear\":3,\"happy\":4,\"sad\":5,\"surprise\":6}\n",
    "df['target'] = df['Total Evaluation'].map(mapping_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "s1 = re.compile('\\n')\n",
    "s1 = re.compile('\\n')\n",
    "\n",
    "def remove_characters(sentence, lower=True):\n",
    "    sentence = s1.sub(' ', str(sentence))\n",
    "    if lower:\n",
    "        sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "df['text'] = df['text'].map(remove_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['not_neutral'] = 1 - df['neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = '/workspace/ETRI/KEMDy20/wav/'\n",
    "for i in range(len(df)):\n",
    "    SegmentID = df.iloc[i,1]\n",
    "    tmp_dir = wav_dir + \"Session\" + SegmentID[4:6] +\"/\" + SegmentID + \".wav\"\n",
    "    df.loc[i,'wav_dir'] = tmp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [['Sess01','Sess02','Sess03','Sess04','Sess05','Sess06','Sess07','Sess08'],\n",
    "['Sess09','Sess10','Sess11','Sess12','Sess13','Sess14','Sess15','Sess16'],\n",
    "['Sess17','Sess18','Sess19','Sess20','Sess21','Sess22','Sess23','Sess24'],\n",
    "['Sess25','Sess26','Sess27','Sess28','Sess29','Sess30','Sess31','Sess32'],\n",
    "['Sess33','Sess34','Sess35','Sess36','Sess37','Sess38','Sess39','Sess40']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = '|'.join(folds[fold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['Segment ID'].str.contains(test_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =df[df['Segment ID'].str.contains(test_list) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.dropna( subset=['Total Evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "input_column = \"wav_dir\"\n",
    "output_column = \"Total Evaluation\"\n",
    "\n",
    "model_name_or_path = \"facebook/wav2vec2-base-960h\"\n",
    "pooling_mode = \"mean\"\n",
    "\n",
    "audio_config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=\"wav2vec2_clf\",\n",
    ")\n",
    "setattr(audio_config, 'pooling_mode', pooling_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "print(f\"The target sampling rate: {target_sampling_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path):\n",
    "    speech_array, sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    input_values = processor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\").input_values\n",
    "    return input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "  def __init__(self, subjects, df, tokenizer, max_len,wav_dir,max_wv_len):\n",
    "    self.subjects = subjects\n",
    "    self.df = df\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "    self.wav_dir = wav_dir\n",
    "    self.max_wv_len = max_wv_len\n",
    "  def __len__(self):\n",
    "    return len(self.subjects)\n",
    "  def __getitem__(self, item):\n",
    "    subject = str(self.subjects[item])\n",
    "    target = self.df.loc[item,['neutral','not_neutral']].values.astype('float')\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      subject,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    wav_data = speech_file_to_array_fn(self.wav_dir[item])\n",
    "    if wav_data.size(-1) > self.max_wv_len:\n",
    "      wav_data = wav_data[:, :self.max_wv_len]\n",
    "    else:\n",
    "      k = self.max_wv_len // wav_data.size(-1)\n",
    "      tmp = torch.zeros(self.max_wv_len - k * wav_data.size(-1)).unsqueeze(0)\n",
    "      tmp2 = wav_data\n",
    "      for i in range(k-1):\n",
    "        wav_data = torch.cat([wav_data,tmp2], dim=1) \n",
    "      wav_data = torch.cat([wav_data,tmp], dim=1) \n",
    "\n",
    "    return {\n",
    "      'subject_text': subject,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.float32),\n",
    "      'wav_data': wav_data.flatten(),\n",
    "    }\n",
    "def create_data_loader(df, tokenizer, max_len, max_wv_len, batch_size, shuffle_=False, valid=False):\n",
    "  ds = SentimentDataset(\n",
    "    subjects=df.text.to_numpy(),\n",
    "    df=df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len,\n",
    "    wav_dir=df.wav_dir.to_numpy(),\n",
    "    max_wv_len = max_wv_len,\n",
    "  )\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    shuffle = shuffle_\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_review_acc(pred, label):\n",
    "    _, idx = pred.max(1)\n",
    "    \n",
    "    acc = torch.eq(idx, label).sum().item() / idx.size()[0] \n",
    "    x = label.cpu().numpy()\n",
    "    y = idx.cpu().numpy()\n",
    "    f1_acc = f1_score(x, y, average='macro')\n",
    "    return acc,f1_acc\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_data_loader = create_data_loader(train, tokenizer, MAX_LEN, MAX_WV_LEN, BATCH_SIZE, shuffle_=True)\n",
    "test_data_loader = create_data_loader(test, tokenizer, MAX_LEN, MAX_WV_LEN, 1, valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "  def __init__(self, n_classes,audio_config):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = AutoModel.from_pretrained(model_name)\n",
    "    self.drop = nn.Dropout(p=0.1)\n",
    "    self.audio_config = audio_config\n",
    "    self.pooling_mode = audio_config.pooling_mode\n",
    "    self.wav2vec2 = Wav2Vec2Model(audio_config)\n",
    "    def get_cls(target_size= n_classes):\n",
    "      return nn.Sequential(\n",
    "          nn.Linear(self.bert.config.hidden_size + self.audio_config.hidden_size, self.bert.config.hidden_size + self.audio_config.hidden_size),\n",
    "          nn.LayerNorm(self.bert.config.hidden_size + self.audio_config.hidden_size),\n",
    "          nn.Dropout(p = 0.1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(self.bert.config.hidden_size + self.audio_config.hidden_size, target_size),\n",
    "      )  \n",
    "    self.cls = get_cls(n_classes)\n",
    "\n",
    "\n",
    "\n",
    "  def freeze_feature_extractor(self):\n",
    "      self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "  def merged_strategy(\n",
    "          self,\n",
    "          hidden_states,\n",
    "          mode=\"mean\"\n",
    "  ):\n",
    "      if mode == \"mean\":\n",
    "          outputs = torch.mean(hidden_states, dim=1)\n",
    "      elif mode == \"sum\":\n",
    "          outputs = torch.sum(hidden_states, dim=1)\n",
    "      elif mode == \"max\":\n",
    "          outputs = torch.max(hidden_states, dim=1)[0]\n",
    "      else:\n",
    "          raise Exception(\n",
    "              \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "      return outputs\n",
    "\n",
    "  def forward(self, input_ids, attention_mask,input_values,\n",
    "            audio_attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      return_dict=False\n",
    "    )\n",
    "    output = self.drop(pooled_output)\n",
    "\n",
    "    return_dict = return_dict if return_dict is not None else self.audio_config.use_return_dict\n",
    "    output2 = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=audio_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "    hidden_states = output2[0]\n",
    "    hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "\n",
    "    output2 = self.drop(hidden_states)\n",
    "\n",
    "    output = torch.cat([output,output2],1) \n",
    "    out = self.cls(output)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "EPOCHS = 20\n",
    "model = SentimentClassifier(n_classes=2,audio_config = audio_config).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=int(total_steps*0.1),\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "nSamples = train.target.value_counts().sort_index().tolist()\n",
    "num = 0\n",
    "for target in targets:\n",
    "    nSamples[num] *=train[target].mean()\n",
    "    num +=1\n",
    "    \n",
    "normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n",
    "for i in range(2,len(normedWeights)):\n",
    "  normedWeights[1] += normedWeights[i]\n",
    "normedWeights = normedWeights[:2]\n",
    "\n",
    "normedWeights = torch.FloatTensor(normedWeights).to(device)\n",
    "\n",
    "\n",
    "loss_fn = nn.MultiLabelSoftMarginLoss(weight=normedWeights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
    "\n",
    "  batch_time = AverageMeter()     \n",
    "  data_time = AverageMeter()      \n",
    "  losses = AverageMeter()         \n",
    "  accuracies = AverageMeter()\n",
    "  f1_accuracies = AverageMeter()\n",
    "  \n",
    "  sent_count = AverageMeter()   \n",
    "    \n",
    "\n",
    "  start = end = time.time()\n",
    "\n",
    "  model = model.train()\n",
    "  correct_predictions = 0\n",
    "  for step,d in enumerate(data_loader):\n",
    "    data_time.update(time.time() - end)\n",
    "    batch_size = d[\"input_ids\"].size(0) \n",
    "    wav_data = d[\"wav_data\"].to(device)\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      input_values=wav_data,\n",
    "    )\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    _, targets_max = torch.max(targets, dim=1)\n",
    "    correct_predictions += torch.sum(preds == targets_max)\n",
    "    losses.update(loss.item(), batch_size)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    sent_count.update(batch_size)\n",
    "    if step % 50 == 0 or step == (len(data_loader)-1):\n",
    "                acc,f1_acc = calc_review_acc(outputs, targets_max)\n",
    "                accuracies.update(acc, batch_size)\n",
    "                f1_accuracies.update(f1_acc, batch_size)\n",
    "\n",
    "                \n",
    "                print('Epoch: [{0}][{1}/{2}] '\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.val:.3f}({loss.avg:.3f}) '\n",
    "                      'Acc: {acc.val:.3f}({acc.avg:.3f}) '   \n",
    "                      'f1_Acc: {f1_acc.val:.3f}({f1_acc.avg:.3f}) '           \n",
    "                      'sent/s {sent_s:.0f} '\n",
    "                      .format(\n",
    "                      epoch, step+1, len(data_loader),\n",
    "                      data_time=data_time, loss=losses,\n",
    "                      acc=accuracies,\n",
    "                      f1_acc=f1_accuracies,\n",
    "                      remain=timeSince(start, float(step+1)/len(data_loader)),\n",
    "                      sent_s=sent_count.avg/batch_time.avg\n",
    "                      ))\n",
    "\n",
    "  return correct_predictions.double() / n_examples, losses.avg\n",
    "\n",
    "def validate(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  outputs_arr = []\n",
    "  preds_arr = []\n",
    "  targets_max_arr = []\n",
    "  correct_predictions = 0\n",
    "  for d in tqdm(data_loader):\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    wav_data = d[\"wav_data\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      input_values=wav_data,\n",
    "    )\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    outputs_arr.append(outputs.cpu().detach().numpy()[0])\n",
    "    preds_arr.append(preds.cpu().numpy())\n",
    "    \n",
    "    loss = loss_fn(outputs, targets)\n",
    "    _, targets_max = torch.max(targets, dim=1)\n",
    "    correct_predictions += torch.sum(preds == targets_max)\n",
    "    targets_max_arr.append(targets_max.cpu().numpy())\n",
    "    losses.append(loss.item())\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses), outputs_arr, preds_arr, targets_max_arr\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  print('-' * 10)\n",
    "  print(f'Epoch {epoch}/{EPOCHS-1}')\n",
    "  print('-' * 10)\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer, \n",
    "    device,\n",
    "    scheduler,\n",
    "    len(train)\n",
    "  )\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "  print(\"\")\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_acc, validate_loss, outputs_arr, preds_arr, targets_max_arr= validate(\n",
    "    model,\n",
    "    test_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = pd.DataFrame()\n",
    "k = 0\n",
    "for i in preds_arr:\n",
    "    tempdf.loc[k,'pred'] = i[0]\n",
    "    k +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['neutral',\n",
    "    'angry',\n",
    "    'disgust',\n",
    "    'fear',\n",
    "    'happy',\n",
    "    'sad',\n",
    "    'surprise']\n",
    "df = pd.read_csv('/workspace/ETRI/new_data.csv')\n",
    "df = df[['Segment ID','Total Evaluation','text','max_count']+ targets]\n",
    "mapping_info = {\"neutral\":0,\"angry\":1,\"disgust\":2,\"fear\":3,\"happy\":4,\"sad\":5,\"surprise\":6}\n",
    "df['target'] = df['Total Evaluation'].map(mapping_info)\n",
    "df['text'] = df['text'].map(remove_characters)\n",
    "wav_dir = '/workspace/ETRI/KEMDy20/wav/'\n",
    "for i in range(len(df)):\n",
    "    SegmentID = df.iloc[i,0]\n",
    "    tmp_dir = wav_dir + \"Session\" + SegmentID[4:6] +\"/\" + SegmentID + \".wav\"\n",
    "    df.loc[i,'wav_dir'] = tmp_dir\n",
    "\n",
    "test_list = '|'.join(folds[fold])\n",
    "test = df[df['Segment ID'].str.contains(test_list)]\n",
    "train =df[df['Segment ID'].str.contains(test_list) == False]\n",
    "train = train[train['Total Evaluation'].str.contains('neutral') == False]\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "train=train.dropna( subset=['Total Evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 6\n",
    "input_column = \"wav_dir\"\n",
    "output_column = \"Total Evaluation\"\n",
    "\n",
    "model_name_or_path = \"facebook/wav2vec2-base-960h\"\n",
    "pooling_mode = \"mean\"\n",
    "\n",
    "audio_config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=\"wav2vec2_clf\",\n",
    ")\n",
    "setattr(audio_config, 'pooling_mode', pooling_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "print(f\"The target sampling rate: {target_sampling_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path):\n",
    "    speech_array, sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    input_values = processor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\").input_values\n",
    "    return input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "  def __init__(self, subjects, df, tokenizer, max_len,wav_dir,max_wv_len):\n",
    "    self.subjects = subjects\n",
    "    self.df = df\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "    self.wav_dir = wav_dir\n",
    "    self.max_wv_len = max_wv_len\n",
    "  def __len__(self):\n",
    "    return len(self.subjects)\n",
    "  def __getitem__(self, item):\n",
    "    subject = str(self.subjects[item])\n",
    "    target = self.df.iloc[item,5:-2].values.astype('float')\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      subject,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    wav_data = speech_file_to_array_fn(self.wav_dir[item])\n",
    "    if wav_data.size(-1) > self.max_wv_len:\n",
    "      wav_data = wav_data[:, :self.max_wv_len]\n",
    "    else:\n",
    "      k = self.max_wv_len // wav_data.size(-1)\n",
    "      tmp = torch.zeros(self.max_wv_len - k * wav_data.size(-1)).unsqueeze(0)\n",
    "      tmp2 = wav_data\n",
    "      for i in range(k-1):\n",
    "        wav_data = torch.cat([wav_data,tmp2], dim=1) \n",
    "      wav_data = torch.cat([wav_data,tmp], dim=1) \n",
    "\n",
    "    return {\n",
    "      'subject_text': subject,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.float32),\n",
    "      'wav_data': wav_data.flatten(),\n",
    "    }\n",
    "def create_data_loader(df, tokenizer, max_len, max_wv_len, batch_size, shuffle_=False, valid=False):\n",
    "  ds = SentimentDataset(\n",
    "    subjects=df.text.to_numpy(),\n",
    "    df=df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len,\n",
    "    wav_dir=df.wav_dir.to_numpy(),\n",
    "    max_wv_len = max_wv_len,\n",
    "  )\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    shuffle = shuffle_\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_data_loader = create_data_loader(train, tokenizer, MAX_LEN, MAX_WV_LEN, BATCH_SIZE, shuffle_=True)\n",
    "test_data_loader = create_data_loader(test, tokenizer, MAX_LEN, MAX_WV_LEN, 1, valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "  def __init__(self, n_classes,audio_config):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = AutoModel.from_pretrained(model_name)\n",
    "    self.drop = nn.Dropout(p=0.1)\n",
    "    self.audio_config = audio_config\n",
    "    self.pooling_mode = audio_config.pooling_mode\n",
    "    self.wav2vec2 = Wav2Vec2Model(audio_config)\n",
    "    def get_cls(target_size= n_classes):\n",
    "      return nn.Sequential(\n",
    "          nn.Linear(self.bert.config.hidden_size + self.audio_config.hidden_size, self.bert.config.hidden_size + self.audio_config.hidden_size),\n",
    "          nn.LayerNorm(self.bert.config.hidden_size + self.audio_config.hidden_size),\n",
    "          nn.Dropout(p = 0.1),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(self.bert.config.hidden_size + self.audio_config.hidden_size, target_size),\n",
    "      )  \n",
    "    self.cls = get_cls(n_classes)\n",
    "\n",
    "\n",
    "\n",
    "  def freeze_feature_extractor(self):\n",
    "      self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "  def merged_strategy(\n",
    "          self,\n",
    "          hidden_states,\n",
    "          mode=\"mean\"\n",
    "  ):\n",
    "      if mode == \"mean\":\n",
    "          outputs = torch.mean(hidden_states, dim=1)\n",
    "      elif mode == \"sum\":\n",
    "          outputs = torch.sum(hidden_states, dim=1)\n",
    "      elif mode == \"max\":\n",
    "          outputs = torch.max(hidden_states, dim=1)[0]\n",
    "      else:\n",
    "          raise Exception(\n",
    "              \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "      return outputs\n",
    "\n",
    "  def forward(self, input_ids, attention_mask,input_values,\n",
    "            audio_attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      return_dict=False\n",
    "    )\n",
    "    output = self.drop(pooled_output)\n",
    "\n",
    "    return_dict = return_dict if return_dict is not None else self.audio_config.use_return_dict\n",
    "    output2 = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=audio_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "    hidden_states = output2[0]\n",
    "    hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "\n",
    "    output2 = self.drop(hidden_states)\n",
    "\n",
    "    output = torch.cat([output,output2],1) \n",
    "    out = self.cls(output)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = SentimentClassifier(n_classes=6,audio_config = audio_config).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=int(total_steps*0.1),\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "targets = [\n",
    "    'angry',\n",
    "    'disgust',\n",
    "    'fear',\n",
    "    'happy',\n",
    "    'sad',\n",
    "    'surprise']\n",
    "\n",
    "nSamples = train.target.value_counts().sort_index().tolist()\n",
    "num = 0\n",
    "for target in targets:\n",
    "    nSamples[num] *=train[target].mean()\n",
    "    num +=1\n",
    "    \n",
    "normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n",
    "normedWeights = torch.FloatTensor(normedWeights).to(device)\n",
    "\n",
    "\n",
    "loss_fn = nn.MultiLabelSoftMarginLoss(weight=normedWeights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  print('-' * 10)\n",
    "  print(f'Epoch {epoch}/{EPOCHS-1}')\n",
    "  print('-' * 10)\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer, \n",
    "    device,\n",
    "    scheduler,\n",
    "    len(train)\n",
    "  )\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "  print(\"\")\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_acc, validate_loss, outputs_arr, preds_arr, targets_max_arr= validate(\n",
    "    model,\n",
    "    test_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf2 = pd.DataFrame()\n",
    "k = 0\n",
    "for i in preds_arr:\n",
    "    tempdf2.loc[k,'pred'] = i[0]\n",
    "    k +=1\n",
    "\n",
    "for i in range(len(tempdf)):\n",
    "    if tempdf.loc[i,'pred'] == 1:\n",
    "        tempdf.loc[i,'pred'] = tempdf2.loc[i,\"pred\"] + 1\n",
    "mapping_info = {\"neutral\":0,\"angry\":1,\"disgust\":2,\"fear\":3,\"happy\":4,\"sad\":5,\"surprise\":6}\n",
    "df['target'] = df['Total Evaluation'].map(mapping_info)\n",
    "\n",
    "test_list = '|'.join(folds[fold])\n",
    "train =df[df['Segment ID'].str.contains(test_list) == False]\n",
    "valid = df[df['Segment ID'].str.contains(test_list)]\n",
    "\n",
    "X = valid['target'].to_list()\n",
    "y = tempdf['pred'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y, X, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, X, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752659724176752"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8522727272727273"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_accuracy_score(y,X)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
